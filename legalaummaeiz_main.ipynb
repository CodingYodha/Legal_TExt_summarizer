{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:44:38.824520Z",
     "iopub.status.busy": "2025-03-30T07:44:38.824105Z",
     "iopub.status.idle": "2025-03-30T07:44:38.944792Z",
     "shell.execute_reply": "2025-03-30T07:44:38.943582Z",
     "shell.execute_reply.started": "2025-03-30T07:44:38.824480Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Legal_TExt_summarizer' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/CodingYodha/Legal_TExt_summarizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:44:58.022277Z",
     "iopub.status.busy": "2025-03-30T07:44:58.021901Z",
     "iopub.status.idle": "2025-03-30T07:45:20.399845Z",
     "shell.execute_reply": "2025-03-30T07:45:20.398871Z",
     "shell.execute_reply.started": "2025-03-30T07:44:58.022250Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c18da72f3bb4378996af34058ee74e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef0a5a6b60e4bcda677db16ce2a0722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7e58d6150245319f5a7ec1a1dcfd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f569e76f7448558a20112fd1dba995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24ce8bddb014d5d941d1fda56f9bfaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe84f65c9184152a6273128b3322ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T07:44:47.388690Z",
     "iopub.status.idle": "2025-03-30T07:44:47.388931Z",
     "shell.execute_reply": "2025-03-30T07:44:47.388833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T07:44:47.389750Z",
     "iopub.status.idle": "2025-03-30T07:44:47.390139Z",
     "shell.execute_reply": "2025-03-30T07:44:47.389959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:23.562172Z",
     "iopub.status.busy": "2025-03-30T07:45:23.561510Z",
     "iopub.status.idle": "2025-03-30T07:45:24.693895Z",
     "shell.execute_reply": "2025-03-30T07:45:24.693216Z",
     "shell.execute_reply.started": "2025-03-30T07:45:23.562137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:27.938116Z",
     "iopub.status.busy": "2025-03-30T07:45:27.937493Z",
     "iopub.status.idle": "2025-03-30T07:45:30.088490Z",
     "shell.execute_reply": "2025-03-30T07:45:30.087862Z",
     "shell.execute_reply.started": "2025-03-30T07:45:27.938085Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bb4baad92e472a8830eb40f1d1e75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd98c85857a4130bf69e38f5b72df40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\" ,\n",
    "                       data_files = {\"train\":\"/kaggle/input/erwtwret/legal_train.csv\" ,\n",
    "                                     \"validation\":\"/kaggle/input/erwtwret/legal_val.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:33.154761Z",
     "iopub.status.busy": "2025-03-30T07:45:33.154441Z",
     "iopub.status.idle": "2025-03-30T07:45:35.175717Z",
     "shell.execute_reply": "2025-03-30T07:45:35.174770Z",
     "shell.execute_reply.started": "2025-03-30T07:45:33.154733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\",\n",
    "                                                    use_cache = False) #disables caching for checkpointing)\n",
    "model.gradient_checkpointing_enable()#reduces memory by 60-70%\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:36.440426Z",
     "iopub.status.busy": "2025-03-30T07:45:36.440116Z",
     "iopub.status.idle": "2025-03-30T07:45:45.989503Z",
     "shell.execute_reply": "2025-03-30T07:45:45.988798Z",
     "shell.execute_reply.started": "2025-03-30T07:45:36.440401Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4b686e38de471e8bbd6c691573a561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7082 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5fe1aa020b48e5826adf5868b1ddd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1771 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs =  examples[\"text\"]\n",
    "    targets =  examples[\"summary\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        # padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=256,\n",
    "            truncation = True,\n",
    "            # padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "barts max input size is 1024 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datacollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:47.483371Z",
     "iopub.status.busy": "2025-03-30T07:45:47.483049Z",
     "iopub.status.idle": "2025-03-30T07:45:47.487131Z",
     "shell.execute_reply": "2025-03-30T07:45:47.486262Z",
     "shell.execute_reply.started": "2025-03-30T07:45:47.483345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer ,\n",
    "                                       model = model,\n",
    "                                      padding=\"longest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "purpose of data collator - dynamic padding the batches during training for efficient memeory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training arguements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:49.800822Z",
     "iopub.status.busy": "2025-03-30T07:45:49.800489Z",
     "iopub.status.idle": "2025-03-30T07:45:49.830866Z",
     "shell.execute_reply": "2025-03-30T07:45:49.830089Z",
     "shell.execute_reply.started": "2025-03-30T07:45:49.800794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "\n",
    "    learning_rate = 3e-5,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=500,\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 3,\n",
    "    fp16=True, #use GPU mixed precision\n",
    "    logging_steps=50,\n",
    "    report_to = \"none\",\n",
    "    gradient_accumulation_steps=2,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fp16=True: Speeds up training on GPUs with Tensor Cores (NVIDIA).\n",
    "\n",
    "per_device_batch_size: Adjust based on GPU memory (start with 2-8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:53.632692Z",
     "iopub.status.busy": "2025-03-30T07:45:53.632364Z",
     "iopub.status.idle": "2025-03-30T07:45:53.636049Z",
     "shell.execute_reply": "2025-03-30T07:45:53.635170Z",
     "shell.execute_reply.started": "2025-03-30T07:45:53.632664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "\n",
    "# # Clear cached memory\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Reduce fragmentation\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:54.578731Z",
     "iopub.status.busy": "2025-03-30T07:45:54.578445Z",
     "iopub.status.idle": "2025-03-30T07:45:54.582250Z",
     "shell.execute_reply": "2025-03-30T07:45:54.581383Z",
     "shell.execute_reply.started": "2025-03-30T07:45:54.578709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PYTORCH_CUDA_ALLOC_CONF=expandable_segments=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:55.350139Z",
     "iopub.status.busy": "2025-03-30T07:45:55.349788Z",
     "iopub.status.idle": "2025-03-30T07:45:55.795690Z",
     "shell.execute_reply": "2025-03-30T07:45:55.795041Z",
     "shell.execute_reply.started": "2025-03-30T07:45:55.350112Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-cf7d7f3da60b>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(5000)),\n",
    "    eval_dataset = tokenized_dataset[\"validation\"],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:57.287078Z",
     "iopub.status.busy": "2025-03-30T07:45:57.286769Z",
     "iopub.status.idle": "2025-03-30T07:45:57.292195Z",
     "shell.execute_reply": "2025-03-30T07:45:57.291292Z",
     "shell.execute_reply.started": "2025-03-30T07:45:57.287055Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU device: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:58.497959Z",
     "iopub.status.busy": "2025-03-30T07:45:58.497635Z",
     "iopub.status.idle": "2025-03-30T07:45:58.501389Z",
     "shell.execute_reply": "2025-03-30T07:45:58.500474Z",
     "shell.execute_reply.started": "2025-03-30T07:45:58.497933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Move model to GPU\n",
    "# model = model.to(\"cuda\")\n",
    "\n",
    "# # Verify model is on GPU\n",
    "# print(next(model.parameters()).device)  # Should output \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T07:45:59.891269Z",
     "iopub.status.busy": "2025-03-30T07:45:59.890912Z",
     "iopub.status.idle": "2025-03-30T08:21:51.021504Z",
     "shell.execute_reply": "2025-03-30T08:21:51.020764Z",
     "shell.execute_reply.started": "2025-03-30T07:45:59.891240Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "CUDA memory allocated: 3.25 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='5250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5250/5250 35:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.987600</td>\n",
       "      <td>2.905222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.328600</td>\n",
       "      <td>2.832303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.812400</td>\n",
       "      <td>2.882676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5250, training_loss=2.469088685535249, metrics={'train_runtime': 2150.7795, 'train_samples_per_second': 9.764, 'train_steps_per_second': 2.441, 'total_flos': 2582037411397632.0, 'train_loss': 2.469088685535249, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset = tokenized_dataset[\"train\"].shuffle().select(range(7000))\n",
    "\n",
    "# Force reinitialization of Trainer internals\n",
    "trainer.train_dataset = small_dataset\n",
    "trainer._train_dataloader = None  # Reset dataloader\n",
    "\n",
    "\n",
    "# Verify GPU usage\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"CUDA memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T08:23:59.804718Z",
     "iopub.status.busy": "2025-03-30T08:23:59.804390Z",
     "iopub.status.idle": "2025-03-30T08:25:22.970377Z",
     "shell.execute_reply": "2025-03-30T08:25:22.969431Z",
     "shell.execute_reply.started": "2025-03-30T08:23:59.804694Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip file contains: ['config.json', 'tokenizer_config.json', 'model.safetensors', 'generation_config.json', 'vocab.json', 'merges.txt', 'special_tokens_map.json']\n",
      "\n",
      "✅ Download ready! Click the 📁 icon in Kaggle's sidebar →\n",
      "→ Output → legal_bart_summarizer.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# 1. Save model & tokenizer (if not already saved)\n",
    "model.save_pretrained(\"legal_bart_summarizer\")\n",
    "tokenizer.save_pretrained(\"legal_bart_summarizer\")\n",
    "\n",
    "# 2. Create zip archive\n",
    "shutil.make_archive(\n",
    "    base_name=\"/kaggle/working/legal_bart_summarizer\",  # Save in Kaggle's working directory\n",
    "    format=\"zip\",\n",
    "    root_dir=\"legal_bart_summarizer\",\n",
    ")\n",
    "\n",
    "# 3. Verify zip contents\n",
    "with ZipFile(\"/kaggle/working/legal_bart_summarizer.zip\", \"r\") as zip:\n",
    "    print(\"Zip file contains:\", zip.namelist())\n",
    "\n",
    "print(\"\\n✅ Download ready! Click the 📁 icon in Kaggle's sidebar →\")\n",
    "print(\"→ Output → legal_bart_summarizer.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T07:44:47.403173Z",
     "iopub.status.idle": "2025-03-30T07:44:47.403466Z",
     "shell.execute_reply": "2025-03-30T07:44:47.403365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# small_dataset = tokenized_dataset[\"train\"].shuffle().select(range(2000))\n",
    "# trainer.train_dataset = small_dataset\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T07:44:47.404048Z",
     "iopub.status.idle": "2025-03-30T07:44:47.404387Z",
     "shell.execute_reply": "2025-03-30T07:44:47.404284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"legal_bart_summarizer\")\n",
    "tokenizer.save_pretrained(\"legal_bart_summarizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T08:25:49.751407Z",
     "iopub.status.busy": "2025-03-30T08:25:49.751125Z",
     "iopub.status.idle": "2025-03-30T08:25:50.764166Z",
     "shell.execute_reply": "2025-03-30T08:25:50.763235Z",
     "shell.execute_reply.started": "2025-03-30T08:25:49.751387Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load model and tokenizer from the saved directory\n",
    "model = BartForConditionalGeneration.from_pretrained(\"legal_bart_summarizer\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"legal_bart_summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T08:25:52.487100Z",
     "iopub.status.busy": "2025-03-30T08:25:52.486656Z",
     "iopub.status.idle": "2025-03-30T08:25:58.915736Z",
     "shell.execute_reply": "2025-03-30T08:25:58.914928Z",
     "shell.execute_reply.started": "2025-03-30T08:25:52.487061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1527: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question was whether the 6 law reports were newspapers within the meaning of the Act.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"HELD: The Court was concerned with the narrow question whether the six law reports aforementioned being published by the 1st respondent were \n",
    "newspapers within the meaning of the Act and whether the employees engaged in their \n",
    "production or distribution were entitled to the benefit of the orders made by the Central Government on the basis of the Palekar Award.\n",
    "\"\"\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=256)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T07:44:47.406894Z",
     "iopub.status.idle": "2025-03-30T07:44:47.407229Z",
     "shell.execute_reply": "2025-03-30T07:44:47.407070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T07:44:47.407752Z",
     "iopub.status.idle": "2025-03-30T07:44:47.408102Z",
     "shell.execute_reply": "2025-03-30T07:44:47.407924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"Shiva7706/legal-bart-summarizer\")\n",
    "tokenizer.push_to_hub(\"Shiva7706/legal-bart-summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, display\n",
    "\n",
    "# Path to your zip file\n",
    "zip_path = \"/kaggle/working/legal_bart_summarizer.zip\"\n",
    "\n",
    "# Create a downloadable link\n",
    "display(FileLink(zip_path, result_html_prefix=\"⬇️ Click to download: \"))\n",
    "\n",
    "# Verify file exists\n",
    "!ls -lh /kaggle/working/legal_bart_summarizer.zip"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7001726,
     "sourceId": 11213010,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
